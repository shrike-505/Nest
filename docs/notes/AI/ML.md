---
    comments: true
    tags: 
        - 大三上笔记
---

# 机器学习

> 任课教师：赵洲
> 参考：《机器学习》（周志华），https://note.gopoux.cc/cs/ml/review/

!!! note "Homework"
    [HW1](./assets/MLhw1.pdf)

## Terminology

- 特征（Feature）：又称属性（Attribute）
- 属性值：属性的取值
- 样本（Sample）/示例（Instance）：数据集中的每一行记录（一个对象的输入）**不含标记**
- 样本维度：样本的属性个数
- 特征张成的空间：属性空间/输入空间
- 标记张成的空间：输出空间
- 样例（Example）：示例+标记
- 任务：
    - 预测任务：根据标记的取值情况
        - 分类任务：标记是离散值（二分类、多分类）
        - 回归任务：标记是连续值
        - 聚类任务：标记为空值，对示例进行分组
    - 监督学习：所有 Instance 都有标记（分类、回归）
    - 无监督学习：所有 Instance 都没有标记（聚类）
    - 半监督学习：少量 Instance 有标记，大量没有标记

机器学习的根本目标是提高模型的**泛化能力**（Generalization），即在未见过的数据上也能有良好的表现——依靠历史数据来逼近泛化能力（假设历史和未来**独立同分布**）。

归纳偏好：学习过程中对某种类型假设的偏好

No free lunch theorem：如果算法 A 在某些问题上表现优于算法 B，那么必然存在另一些问题，算法 B 优于算法 A。

## 模型评估与选择

![模型评估与选择](./assets/ML1.png)

!!! note "错误率与误差"
    - 错误率（Error Rate）：分类错误的样本数占总样本数的比例
    - 误差（Error）：样本真实输出与预测输出之间的差距

- 过拟合：模型学习能力过强，将训练样本本身的某些特质都当作所有样本的一般性质，导致泛化能力下降
    - Sol：优化目标加正则项，early stopping
- 欠拟合：对训练样本的一般性质都没有学到
    - Sol：增加模型复杂度，决策树（拓展分支），神经网络（增加训练轮数）

??? note "将数据集分成训练集和测试集（评估方法）"
    - 留出法（Hold-out）：将数据集随机分成训练集和测试集
        - 划分尽可能保持数据分布的一致性
        - 分层采样（Stratified Sampling）：保持类别比例一致
    - 交叉验证（Cross Validation）：将数据集分成 k 份，轮流用 k-1 份做训练集，1 份做测试集，最终得到 k 个测试结果，取平均值作为返回结果
    - 自助法
        - ![自助法](./assets/ML2.png)

在进行模型评估与选择时，除了对适用学习算法进行选择，还需对算法参数进行设定（调参）。

### 经验性能的指标多样：性能度量

对模型泛化能力的评价标准

- 回归任务常用**均方误差**
    - $E(f;D) = \frac{1}{m} \sum_{i=1}^{m} (f(x_i)-y_i)^2$
- 对分类任务，错误率和精度最常用（即分对/错样本占样本总数的比例）
    - 错误率：$E(f;D) = \frac{1}{m} \sum_{i=1}^{m} I(f(x_i) \neq y_i)$
    - 精度：$Acc(f;D) = \frac{1}{m} \sum_{i=1}^{m} I(f(x_i) = y_i) = 1 - E(f;D)$
    - 混淆矩阵：
        - ![混淆矩阵](./assets/ML3.png)
        - 查重率-查准率曲线（P-R Curve）
        - F1值：$F_1 = \frac{2 \cdot P \cdot R}{P + R} = \frac{2 \cdot TP}{样例总数 + TP - TN}$

## 线性模型

对于由 $d$ 个属性描述的 Instance $x$，线性模型试图学得一个通过属性的线性组合来进行预测的函数，即 $f(x) = w_1 x_1 + w_2 x_2 + ... + w_d x_d + b = w \cdot x + b$。其中 $x_i$ 是 $x$ 在第 $i$ 个属性上的取值

- 线性回归
    - 目标：回归任务。最小化均方误差，对于输入属性个数为1的最简单情况，即 $(w^*, b^*) = \arg\min_{w,b} \sum_{i=1}^{m} (y_i - w x_i - b)^2$，亦即最小二乘法。对该式求导即可得到最小值点
    - 对于多元线性回归，数据集 $D$ 表示为一个矩阵 $X$，在 $X^T X$ 满秩或正定的情况下可学得模型 $f(x) = \hat{x}_i^T (X^T X)^{-1} X^T y$，其中 $\hat{x}_i = (x_i, 1)$
        - 现实中常见 $X^T X$ 不满秩的情况，此时可解出多个 $\hat{w}$，它们都能使均方误差最小化。选择哪个解作为输出将由学习算法的归纳偏好决定，常见的做法是引入正则化 (regularization) 项
    - 命 $f(x) = \ln y$，则为对数线性回归
- 对数几率回归
    - 分类任务。对于二分类任务，输出标记 $y \in \{0, 1\}$，而线性回归模型产生的预测值 $z = w^T x + b$ 是实值，于是我们需将预测值转换为 0/1，例如 Heaviside 步函数（$f(z) = 1$ if $z \geq 0$ else $0$），但该函数不可导，不利于优化
    - Sigmoid 函数：$f(z) = \frac{1}{1 + e^{-z}} = \frac{1}{1+e^{w^T x + b}}$
    - 此时可得到 $\ln \frac{y}{1-y} = w^T x + b$，$\frac{y}{1-y}$ 称为几率（odds），加上对数称为对数几率，即对数几率（logit）是输入属性的线性函数
- 线性判别分析
    - 给定训练样本，将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离；在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别
    - 可被视为一种监督降维技术
    - 两类数据同先验，满足高斯分布且协方差相等时，LDA 达到最优分类

### 多分类学习

将多分类任务拆成若干个二分类任务来解决，对每个分类器的预测结果进行集成得到多分类结果

- 一对多（one-vs.-rest, OvR）：对于 $k$ 类分类任务，命某一类别为正例，其余类别为反例，训练 $k$ 个分类器，对于每个样本，每个分类器输出正例的概率，最终概率最大的类别为预测类别。
- 一对一（one-vs.-one, OvO）：对于 $k$ 类分类任务，两两匹配，训练 $k(k-1)/2$ 个二分类器，新样本提交给所有分类器预测，投票被预测最多的结果即为预测结果


## 决策树

TBD

## 神经网络


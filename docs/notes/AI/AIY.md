---
    comments: true
    tags:
        - 大二上笔记
---

# 人工智能引论

> AI导？网安导！  
> 课表里有一门冬8周才开始自学的课  
> 任课教师：黄正行

## 知识表示与推理
### 命题逻辑

离散数学打进来了？

命题：要么真，要么假（得有个 TRuTh ValUe）。

归结法：

- $a \lor b$, $\neg b \lor c$可推出$a \lor c$。

### 谓词逻辑

!!! note "e.g."
    - $king(x)$: $x$是国王，这是一个一元谓词。
    - $Head_on(crown, x)$: $x$戴着皇冠，这是一个二元谓词。

### 知识图谱推理

### 概率图推理

不考

### 因果推理

不考

## 搜索与求解
### 搜索算法基础

- 状态
- 动作
- 状态转移
- 搜索树
- BFS
    - ![BFS](./assets/aiy1.png)
- DFS
    - ![DFS](./assets/aiy2.png)
- 评测标准：
    - 完备性：问题存在解时，算法是否能保证找到解。
    - 最优性：是否能找到最优解。
    - 时间复杂度
    - 空间复杂度

### 启发式搜索

- 定义启发函数$h(n)$，用于评估节点$n$到目标节点的 path 的最小代价。
- 定义评价函数$f(n)$，用于选择节点$n$的下一个节点。

!!! example "贪婪最佳优先搜索"
    - $f(n) = h(n)$

!!! example "A*搜索"
    - $f(n) = g(n) + h(n)$
    - $g(n)$ 为从初始节点到节点$n$的实际代价。（当前最小代价）
    - 性质：
        - 一致性（consistency）：启发函数的一致性指满足条件$ℎ(𝑛)≤𝑐(𝑛, 𝑎, 𝑛')+ℎ(𝑛')$，这里𝑐(𝑛, 𝑎, 𝑛′)表示结点𝑛通过动作a到达其相应的后继结点𝑛′的代价(三角不等式原则)。
        - 可容性（admissible）：对于任意结点𝑛，有$ℎ(𝑛)≤ℎ^∗ (𝑛)$，如果𝑛是目标结点，则有ℎ(𝑛)=0。如表3.3所示， $ℎ^∗ (𝑛)$是从结点𝑛出发到达终止结点所付出的（实际）最小代价。可以这样理解满足可容性的启发函数，启发函数不会过高估计（over-estimate）从结点𝑛到终止结点所应该付出的代价（即估计代价小于等于实际代价）
        - 满足一致性条件的启发函数一定满足可容性条件
        - 如果启发函数是可容的，那么A*搜索算法是最优的。
    !!! note "如果问题和启发函数满足以下条件，则A*搜索算法是完备的"
        - 搜索树中分支数量是有限的，即每个结点的后继结点数量是有限的
        - 单步代价的下界是一个正数
        - 启发函数有下界



### 对抗搜索

- 一方最大化某个利益，另一方最小化某个利益。
- MiniMax
- Alpha-Beta 剪枝

### 蒙特卡洛树搜索

- 使用**上限置信区间算法**

## 机器学习

- 经验风险：训练集中数据产生的损失。
    - 经验风险越小说明学习模型对训练数据拟合程度越好。
- 期望风险：当测试集中存在无穷多数据时产生的损失。
    - 期望风险越小，学习所得模型越好。

![Risk](./assets/aiy3.png)

## 有监督学习

## 无监督学习

学习目标：探究同一数据集的数据分布

## 深度学习

Softmax函数一般用于多分类问题中，其将输入数据 $𝒙_𝒊$ 映射到第 $𝒊$ 个类别的概率 $𝒚_𝒊=softmax(x_i)=\frac{𝒆^(𝒙_𝒊)}{\Sigma_{j=1}^{k} e^(x_j )}$

### 神经网络
#### 前馈神经网络

各个神经元接受前一级的输入，输出给下一级，没有反馈。

!!! note "损失函数"
    假设有 $n$ 个训练数据 $x_i$，每个训练数据 $x_i$ 的真实输出为 $y_i$，模型对 $x_i$ 的预测值为 $\hat{y}_i$
    - 交叉熵损失函数：$L(y, \hat{y}) = -\Sigma_{i=1}^{n} y_i \log(\hat{y}_i)$
    - 均方误差损失函数：$L(y, \hat{y}) = \frac{1}{2} \Sigma_{i=1}^{n} (y_i - \hat{y}_i)^2$
    !!! note "梯度下降算法"
        梯度下降算法是一种迭代优化算法，用于最小化损失函数。

#### 卷积神经网络

全连接层：特征图转化为向量  
分类层：输出识别分类的置信度值

## 可能考 or 考过了的题

> 来源于HZX老师最后两节课

- AlphaGo 用到了：强化学习，蒙特卡洛树搜索。
- 逻辑推理的三个层次：关联，干预，反事实（由易到难）。
- 广度优先搜索、深度优先搜索、蒙特卡洛树搜索属于启发式搜索吗？
- 归纳推理、因果推理、演绎推理的实际案例。
- 归结法证明某个命题是否成立。
- 用命题量词逻辑表示“所有实数的平方大于等于0”。
- **混淆偏差、因果关联、选择偏差的实例选择。**
    - 程序员和格子衬衫：选择偏差
    - 力形所以奋也：力和形是因果关联
    - 地面湿与行人撑伞：混淆偏差
- 哪个方案为导致MiniMax的算法结果改变？
- 博弈树
- 遗憾值计算